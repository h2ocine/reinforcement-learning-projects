{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d462e328",
   "metadata": {},
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the\n",
    " LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee02ab",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, you will code a naive actor-critic algorithm in the tabular case. Then you will tune it using grid search and Bayesian optimization, potentially using the [optuna](https://optuna.readthedocs.io/en/stable/) library.\n",
    "Finally, you will get the best hyper-parameters obtained with both methods and perform a statistical test to see if there is a statistically significant difference between these methods and with respect to naive hyper-parameter values.\n",
    "\n",
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4f3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install 'easypip>=1.2.0'\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")\n",
    "easyinstall(\"optuna\")\n",
    "easyinstall(\"gymnasium\")\n",
    "easyinstall(\"mazemdp\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import hydra\n",
    "import optuna\n",
    "import yaml\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# For visualization\n",
    "os.environ[\"VIDEO_FPS\"] = \"5\"\n",
    "if not os.path.isdir(\"./videos\"):\n",
    "    os.mkdir(\"./videos\")\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5f794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8369e4b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib backend: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from bbrl.utils.chrono import Chrono\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mazemdp.toolbox import sample_categorical\n",
    "from mazemdp.mdp import Mdp\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from functools import partial\n",
    "\n",
    "matplotlib.use(\"TkAgg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a1663",
   "metadata": {},
   "source": [
    "# Step 1: Coding the naive Actor-critic algorithm\n",
    "\n",
    "We consider the naive actor-critic algorithm with a categorical policy.\n",
    "The algorithm learns a critic with the standard temporal difference mechanism\n",
    "using a learning rate $\\alpha_{critic}$.\n",
    "\n",
    "We consider a value-based critic $V(s)$. The extension to an action value function $Q(s,a)$ is straightforward.\n",
    "\n",
    "To update the critic, the algorithm computes the temporal difference error:\n",
    "\n",
    "$$\\delta_t = r(s_t, a_t) + \\gamma V^{(n)}(s_{t+1})-V^{(n)}(s_t).$$\n",
    "\n",
    "Then it applies it to the critic:\n",
    "\n",
    "$$V^{(n+1)}(s_t) = V^{(n)}(s_t) + \\alpha_{critic} \\delta_t.$$\n",
    "\n",
    "To update the actor, the general idea is the same, using the temporal difference error with another learning rate $\\alpha_{actor}$.\n",
    "\n",
    "However, naively applying the same learning rule would not ensure that the probabilities of all actions in a state sum to 1.\n",
    "Besides, when the temporal difference error $\\delta_t$ is negative, it may happen that the probability of an action gets negative or null, which raises an issue when applying renormalization.\n",
    "\n",
    "So, instead of applying the naive rule, we apply the following one:\n",
    "$$ \n",
    "\\pi_{temp}(a_t|s_t) =  \\begin{cases}\n",
    "\\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t & \\mathrm{if } \\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t > 10^{-8}\\\\\n",
    "10^{-8} & \\mathrm{otherwise.} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then we can apply renormalization so that the probabilities of actions still sum to 1, with\n",
    "$$\n",
    "\\forall a, \\pi^{(i+1)}(a|s_t) = \\frac{\\pi_{temp}^{(i+1)}(a|s_t)} {\\sum_{a'} \\pi_{temp}^{(i+1)}(a'|s_t)}\n",
    "$$ with\n",
    "$$ \n",
    "\\pi_{temp}^{(i+1)}(a|s_t) =  \\begin{cases}\n",
    "\\pi_{temp}(a|s_t) & \\mathrm{if } a = a_t\\\\\n",
    "\\pi^{(i)}(a|s_t) & \\mathrm{otherwise.} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "### 1. Code the naive actor-critic algorithm as specified above.\n",
    "\n",
    "Some hints:\n",
    "\n",
    "- a good idea to build this code it to take inspiration from the code of Q-learning, to add an actor (a categorical policy), both learning rates,\n",
    "and to take care about the renormalization function.\n",
    "\n",
    "- for the next steps of this lab, having a function to repeatedly call your actor-critic algorithm and save the learning trajectories and\n",
    "norms of the value function is a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28475348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table.maze {\n",
       "    border-collapse: collapse;\n",
       "}\n",
       "\n",
       "td {\n",
       "    text-align: center;\n",
       "}\n",
       "\n",
       "table.maze td.cell {\n",
       "    position: relative;\n",
       "    border: 1px solid black;\n",
       "}\n",
       "\n",
       "table.maze td.cell div.agent {\n",
       "    position: absolute;\n",
       "    left: 25%;\n",
       "    top: 25%;;\n",
       "    width: 50%;\n",
       "    height: 50%;\n",
       "    border-radius: 100%;\n",
       "    background:rgba(69, 100, 186, 0.5);\n",
       "}\n",
       "\n",
       "\n",
       "td.wall {\n",
       "    background: black;\n",
       "}\n",
       "\n",
       "td.terminal {\n",
       "    background: rgb(246, 170, 246);\n",
       "}\n",
       "\n",
       "table.maze table td {\n",
       "    width: .5rem;\n",
       "    height: .5rem;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "table.maze table td.arrow {\n",
       "    color: transparent;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       "table.maze table td.value {\n",
       "    position: relative;\n",
       "    width: 2rem;\n",
       "    height: 2rem;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c00424cc1a34a2ea52f4420a9bc457e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MazeWidget(cells=array([[ 0,  3,  5,  8],\n",
       "       [ 1, -1,  6,  9],\n",
       "       [ 2,  4,  7, -1]]), t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Environment with 20% of walls and no negative reward when hitting a wall\n",
    "env = gym.make(\n",
    "    \"MazeMDP-v0\",\n",
    "    kwargs={\"width\": 4, \"height\": 3, \"ratio\": 0.2, \"hit\": 0.0},\n",
    "    render_mode=\"human\",\n",
    ")\n",
    "env.reset()\n",
    "env.unwrapped.init_draw(\"The maze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dceb7c7b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def naive_actor_critic_with_params(ac_params):\n",
    "    \"\"\"\n",
    "    Naive actor-critic algorithm using parameters from the dictionary.\n",
    "    \n",
    "    Args:\n",
    "        ac_params: Dictionary containing all relevant parameters for the experiment.\n",
    "    \n",
    "    Returns:\n",
    "        policy, value_function, trajectories\n",
    "    \"\"\"\n",
    "    # Extract parameters from the dictionary\n",
    "    env_params = ac_params['mdp']\n",
    "    env = MazeMDPEnv(\n",
    "        width=env_params['width'], \n",
    "        height=env_params['height'], \n",
    "        ratio=env_params['ratio'],\n",
    "        render_mode=env_params['render_mode']\n",
    "    )\n",
    "    \n",
    "    alpha_actor = ac_params['alpha_actor']\n",
    "    alpha_critic = ac_params['alpha_critic']\n",
    "    gamma = 0.99  # Discount factor is not in the dict, so using a default value\n",
    "    nb_episodes = ac_params['nb_episodes']\n",
    "    timeout = ac_params['timeout']\n",
    "    render = ac_params['render']\n",
    "\n",
    "    # Initialize value function V(s) and the policy pi(a|s)\n",
    "    V = np.zeros(env.nb_states)  # Critic (value function V(s))\n",
    "    pi = np.ones((env.nb_states, env.action_space.n)) / env.action_space.n  # Actor (policy pi(a|s))\n",
    "\n",
    "    # Function to renormalize the policy for a given state\n",
    "    def renormalize_policy(pi, s):\n",
    "        pi[s, :] = pi[s, :] / np.sum(pi[s, :])\n",
    "\n",
    "    trajectories = []\n",
    "\n",
    "    for episode in range(nb_episodes):\n",
    "        s, _ = env.reset(uniform=True)\n",
    "        cpt = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (terminated or truncated) and cpt < timeout:\n",
    "            if render:\n",
    "                env.draw_v_pi(V, pi.argmax(axis=1))\n",
    "\n",
    "            # Sample an action from the current policy pi(a|s)\n",
    "            a = sample_categorical(pi[s, :])\n",
    "\n",
    "            # Perform a step in the environment\n",
    "            s_next, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "            # Calculate the temporal difference error (TD error)\n",
    "            delta = r + gamma * V[s_next] * (1 - terminated) - V[s]\n",
    "\n",
    "            # Update the critic (value function)\n",
    "            V[s] = V[s] + alpha_critic * delta\n",
    "\n",
    "            # Update the actor (policy) for the action taken\n",
    "            pi_temp = pi[s, a] + alpha_actor * delta\n",
    "            pi_temp = max(pi_temp, 1e-8)  # Ensure non-negative probability\n",
    "\n",
    "            # Apply renormalization to ensure sum of probabilities is 1\n",
    "            pi[s, a] = pi_temp\n",
    "            renormalize_policy(pi, s)\n",
    "\n",
    "            # Move to the next state\n",
    "            s = s_next\n",
    "            cpt += 1\n",
    "\n",
    "        trajectories.append(cpt)\n",
    "\n",
    "    return pi, V, trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40917c",
   "metadata": {},
   "source": [
    "### 2. Provide a plot function\n",
    "\n",
    "Your plot function should show the evolution through time of number of steps the agent takes to find the reward in the maze.\n",
    "If your algorithm works, this number of steps should decrease through time.\n",
    "\n",
    "Your plot function should also show a mean and a standard deviation (or some more advanced statistics) over a collection of learning runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70039d0b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_multiple_experiments(env, alpha_actor, alpha_critic, gamma, nb_episodes, timeout, n_runs):\n",
    "    \"\"\"\n",
    "    Runs the naive actor-critic algorithm multiple times and returns the results.\n",
    "\n",
    "    Args:\n",
    "        env: The environment.\n",
    "        alpha_actor: Learning rate for the actor.\n",
    "        alpha_critic: Learning rate for the critic.\n",
    "        gamma: Discount factor.\n",
    "        nb_episodes: Number of episodes for each run.\n",
    "        timeout: Maximum number of steps per episode.\n",
    "        n_runs: Number of independent runs.\n",
    "\n",
    "    Returns:\n",
    "        all_trajectories: A 2D array where each row corresponds to the number of steps in each episode for a single run.\n",
    "    \"\"\"\n",
    "    all_trajectories = []\n",
    "\n",
    "    for _ in range(n_runs):\n",
    "        _, _, trajectories = naive_actor_critic(\n",
    "            env, alpha_actor, alpha_critic, gamma, nb_episodes, timeout, render=False\n",
    "        )\n",
    "        all_trajectories.append(trajectories)\n",
    "\n",
    "    return np.array(all_trajectories)\n",
    "\n",
    "\n",
    "def plot_learning_curve(all_trajectories):\n",
    "    \"\"\"\n",
    "    Plots the learning curve showing the evolution of the number of steps the agent takes to find the goal.\n",
    "    Also plots the mean and standard deviation across multiple runs.\n",
    "\n",
    "    Args:\n",
    "        all_trajectories: A 2D array where each row corresponds to the number of steps in each episode for a single run.\n",
    "    \"\"\"\n",
    "    # Calculate mean and standard deviation across runs\n",
    "    mean_steps = np.mean(all_trajectories, axis=0)\n",
    "    std_steps = np.std(all_trajectories, axis=0)\n",
    "\n",
    "    # Plot the mean and standard deviation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(mean_steps, label='Mean number of steps')\n",
    "    plt.fill_between(range(len(mean_steps)), mean_steps - std_steps, mean_steps + std_steps, alpha=0.3, label='Std deviation')\n",
    "\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Number of steps to reach goal')\n",
    "    plt.title('Learning Curve: Evolution of Steps to Reach Goal')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "n_runs = 1  # Number of independent runs\n",
    "\n",
    "# Run multiple experiments and gather results\n",
    "all_trajectories = run_multiple_experiments(env, alpha_actor, alpha_critic, gamma, nb_episodes, timeout, n_runs)\n",
    "\n",
    "# Plot the learning curve with mean and standard deviation\n",
    "plot_learning_curve(all_trajectories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f118a80",
   "metadata": {},
   "source": [
    "## Actor-critic hyper-parameters\n",
    "\n",
    "To represent the hyper-parameters of the experiments performed in this notebook, we suggest using the dictionary below.\n",
    "This dictionary can be read using omegaconf.\n",
    "Using it is not mandatory.\n",
    "You can also change the value of hyper-parameters or environment parameters at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b61b5019",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ac_params = {\n",
    "    \"save_curves\": False,\n",
    "    \"save_heatmap\": True,\n",
    "    \"mdp\": {\n",
    "        \"name\": \"MazeMDP-v0\",\n",
    "        \"width\": 5,\n",
    "        \"height\": 5,\n",
    "        \"ratio\": 0.2,\n",
    "        \"render_mode\": \"rgb_array\",\n",
    "        },\n",
    "        \n",
    "    \"log_dir\": \"./tmp\",\n",
    "    \"video_dir\": \"./tmp/videos\",\n",
    "\n",
    "    \"nb_episodes\": 100,\n",
    "    \"timeout\": 200,\n",
    "    \"render\": True, # True, # \n",
    "    \"nb_repeats\": 5,\n",
    "\n",
    "    \"alpha_critic\": 0.5,\n",
    "    \"alpha_actor\": 0.5,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ea76c",
   "metadata": {},
   "source": [
    "### 3. Test your code\n",
    "\n",
    "Once everything looks OK, save the obtained plot for your lab report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59b395d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "naive_actor_critic() missing 3 required positional arguments: 'alpha_actor', 'alpha_critic', and 'gamma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the naive actor-critic algorithm\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m policy, value_function, trajectories \u001b[38;5;241m=\u001b[39m \u001b[43mnaive_actor_critic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mac_params\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: naive_actor_critic() missing 3 required positional arguments: 'alpha_actor', 'alpha_critic', and 'gamma'"
     ]
    }
   ],
   "source": [
    "# Run the naive actor-critic algorithm\n",
    "policy, value_function, trajectories = naive_actor_critic(\n",
    "    ac_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670cf5a",
   "metadata": {},
   "source": [
    "# Step 2: Tuning hyper-parameters\n",
    "\n",
    "In this part, you have to optimize two hyper-parameters of the actor-critic algorithm, namely the actor and critic learning rates.\n",
    "You have to do so using a simple grid search method and some Bayesian optimization method.\n",
    "For the latter, we suggest using the default sampler from [optuna](https://optuna.readthedocs.io/en/stable/).\n",
    "Follow the above link to understand how optuna works.\n",
    "Note that it also supports grid search and many other hyper-parameters tuning algorithms.\n",
    "\n",
    "You should make sure that the hyper-parameters tuning algorithms that you compare benefit from the same training budget\n",
    "We suggest 400 training runs overall for each method,\n",
    "which means 20 values each for the actor and the critic learning rates in the case of grid search.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "### 1. Perform hyper-parameters tuning with two algorithms as suggested above.\n",
    "\n",
    "### 2. Provide a \"heatmap\" of the norm of the value function given the hyper-parameters, after training for each pair of hyper-parameters.\n",
    "\n",
    "### 3. Collect the value of the best hyper-parameters found with each algorithm. You will need them for Step 3.\n",
    "\n",
    "### 4. Include in your report the heatmaps and the best hyper-parameters found for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9925709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623963b",
   "metadata": {},
   "source": [
    "# Step 3: Statistical tests\n",
    "\n",
    "Now you have to compare the performance of the actor-critic algorithm tuned\n",
    "with all the best hyper-parameters you found before, using statistical tests.\n",
    "\n",
    "The functions below are provided to run Welch's T-test over learning curves.\n",
    "They have been adapted from a github repository: https://github.com/flowersteam/rl_stats\n",
    "You don't need to understand them in detail (though it is always a good idea to try to understand more code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2beb4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import bootstrapped.bootstrap as bs\n",
    "import bootstrapped.compare_functions as bs_compare\n",
    "import bootstrapped.stats_functions as bs_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f1185",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_central_tendency_and_error(id_central, id_error, sample):\n",
    "\n",
    "    try:\n",
    "        id_error = int(id_error)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if id_central == \"mean\":\n",
    "        central = np.nanmean(sample, axis=1)\n",
    "    elif id_central == \"median\":\n",
    "        central = np.nanmedian(sample, axis=1)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if isinstance(id_error, int):\n",
    "        low = np.nanpercentile(sample, q=int((100 - id_error) / 2), axis=1)\n",
    "        high = np.nanpercentile(sample, q=int(100 - (100 - id_error) / 2), axis=1)\n",
    "    elif id_error == \"std\":\n",
    "        low = central - np.nanstd(sample, axis=1)\n",
    "        high = central + np.nanstd(sample, axis=1)\n",
    "    elif id_error == \"sem\":\n",
    "        low = central - np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])\n",
    "        high = central + np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return central, low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f1780",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_test(test_id, data1, data2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute tests comparing data1 and data2 with confidence level alpha\n",
    "    :param test_id: (str) refers to what test should be used\n",
    "    :param data1: (np.ndarray) sample 1\n",
    "    :param data2: (np.ndarray) sample 2\n",
    "    :param alpha: (float) confidence level of the test\n",
    "    :return: (bool) if True, the null hypothesis is rejected\n",
    "    \"\"\"\n",
    "    data1 = data1.squeeze()\n",
    "    data2 = data2.squeeze()\n",
    "    n1 = data1.size\n",
    "    n2 = data2.size\n",
    "\n",
    "    # perform Welch t-test\":\n",
    "    _, p = ttest_ind(data1, data2, equal_var=False)\n",
    "    return p < alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4973d3db",
   "metadata": {},
   "source": [
    "This last function was adapted for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803d52e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_test(perf1, perf2, name1, name2, sample_size=20, downsampling_fact=5, confidence_level=0.01):\n",
    "\n",
    "    perf1 = perf1.transpose()\n",
    "    perf2 = perf2.transpose()\n",
    "    nb_datapoints = perf1.shape[1]\n",
    "    nb_steps = perf1.shape[0]\n",
    "\n",
    "    legend = [name1, name2]\n",
    "\n",
    "    # what do you want to plot ?\n",
    "    id_central = 'mean' # \"median\"  # \n",
    "    id_error = 80  # (percentiles), also: 'std', 'sem'\n",
    "\n",
    "    test_id = \"Welch t-test\"  # recommended\n",
    "    \n",
    "    sample1 = perf1[:, np.random.randint(0, nb_datapoints, sample_size)]\n",
    "    sample2 = perf2[:, np.random.randint(0, nb_datapoints, sample_size)]\n",
    "\n",
    "    steps = np.arange(0, nb_steps, downsampling_fact)\n",
    "    sample1 = sample1[steps, :]\n",
    "    sample2 = sample2[steps, :]\n",
    "\n",
    "    # test\n",
    "    sign_diff = np.zeros([len(steps)])\n",
    "    for i in range(len(steps)):\n",
    "        sign_diff[i] = run_test(\n",
    "            test_id, sample1[i, :], sample2[i, :], alpha=confidence_level\n",
    "        )\n",
    "\n",
    "    central1, low1, high1 = compute_central_tendency_and_error(\n",
    "        id_central, id_error, sample1\n",
    "    )\n",
    "    central2, low2, high2 = compute_central_tendency_and_error(\n",
    "        id_central, id_error, sample2\n",
    "    )\n",
    "\n",
    "    # plot\n",
    "    _, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    lab1 = plt.xlabel(\"training steps\")\n",
    "    lab2 = plt.ylabel(\"performance\")\n",
    "\n",
    "    plt.plot(steps, central1, linewidth=10)\n",
    "    plt.plot(steps, central2, linewidth=10)\n",
    "    plt.fill_between(steps, low1, high1, alpha=0.3)\n",
    "    plt.fill_between(steps, low2, high2, alpha=0.3)\n",
    "    leg = ax.legend(legend, frameon=False)\n",
    "\n",
    "    # plot significative difference as dots\n",
    "    idx = np.argwhere(sign_diff == 1)\n",
    "    y = max(np.nanmax(high1), np.nanmax(high2))\n",
    "    plt.scatter(steps[idx], y * 1.05 * np.ones([idx.size]), s=100, c=\"k\", marker=\"o\")\n",
    "\n",
    "    # style\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(10.0)\n",
    "    ax.spines[\"top\"].set_linewidth(5)\n",
    "    ax.spines[\"right\"].set_linewidth(5)\n",
    "    ax.spines[\"bottom\"].set_linewidth(5)\n",
    "    ax.spines[\"left\"].set_linewidth(5)\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"./{name1}_{name2}.png\", bbox_extra_artists=(leg, lab1, lab2), bbox_inches=\"tight\", dpi=100\n",
    "    )\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9505406",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "As hyper-parameters, you will use:\n",
    "\n",
    "- naive tuning, that is a pair (0.5, 0.5) for the actor and critic learning rates,\n",
    "- the best hyper-parameters you found with the different tuning algorithms you used before.\n",
    "\n",
    "### 1. For each set of hyper-parameters, collect a large dataset of learning curves.\n",
    "\n",
    "We suggest using 150 training episodes.\n",
    "\n",
    "### 2. Perform statistical comparisons\n",
    "\n",
    "- Take two datasets of learning curves obtained with the hyper-parameters sets that you found with different tuning algorithms.\n",
    "- Use the ``` perform_test(...)``` function to compare each possible pair of sets.\n",
    "\n",
    "You should obtain an image for each pair you have tried.\n",
    "In this image, black dots signal the time step where there is a statistically significant difference between two learning curves.\n",
    "\n",
    " ### 3. Conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f0c3c",
   "metadata": {},
   "source": [
    "# Lab report\n",
    "\n",
    "Your report should contain:\n",
    "- your source code (probably this notebook), do not forget to put your names on top of the notebook,\n",
    "- in a separate pdf file with your names in the name of the file:\n",
    "    + a detailed enough description of the choices you have made: the parameters you have set, the libraries you have used, etc.,\n",
    "    + the heatmaps obtained using the hyper-parameters tuning algorithms that you have used,\n",
    "    + the figures resulting from performing Welch's T-test using the best hyper-parameters from the above approaches,\n",
    "    + your conclusion from these experiments.\n",
    "\n",
    "Beyond the elements required in this report, any additional studies will be rewarded.\n",
    "For instance, you can try using a Q-function as critic, using random search as hyper-parameters tuning algorithm,\n",
    "using more challenging environments, etc."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
