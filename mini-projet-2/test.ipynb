{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Prepare the environment\n",
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=False)\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.visu.plot_policies import plot_policy\n",
    "from omegaconf import OmegaConf\n",
    "from td3_ddpg import DDPG, run_ddpg, TD3, run_td3\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from wrappers import FeatureFilterWrapper, ObsTimeExtensionWrapper, ActionTimeExtensionWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/ddpg-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    # Set to true to have an insight on the learned policy\n",
    "    # (but slows down the evaluation a lot!)\n",
    "    \"plot_agents\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 1,\n",
    "        \"n_steps\": 100,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.8,\n",
    "        \"buffer_size\": 1e6,\n",
    "        \"batch_size\": 64,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"max_epochs\": 6_000,\n",
    "        # Minimum number of transitions before learning starts\n",
    "        \"learning_starts\": 10_000,\n",
    "        \"action_noise\": 0.1,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [400, 300],\n",
    "            \"critic_hidden_size\": [400, 300],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "        # \"eps\": 5e-5,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "        # \"eps\": 5e-5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_filter_wrapper_1(env):\n",
    "    \"\"\"Wrapper pour enlever la première feature (par exemple, l'index 1)\"\"\"\n",
    "    return FeatureFilterWrapper(env, 1)\n",
    "\n",
    "def feature_filter_wrapper_3(env):\n",
    "    \"\"\"Wrapper pour enlever la troisième feature (par exemple, l'index 3)\"\"\"\n",
    "    return FeatureFilterWrapper(env, 3)\n",
    "\n",
    "def feature_filter_wrapper_both(env):\n",
    "    \"\"\"Wrapper pour enlever les deux features : première et troisième\"\"\"\n",
    "    return FeatureFilterWrapper(FeatureFilterWrapper(env, 3), 1)\n",
    "\n",
    "\n",
    "# Fonctions pour ObsTimeExtensionWrapper et ActionTimeExtensionWrapper avec feature filtering\n",
    "def obs_time_extension_wrapper_dx(env):\n",
    "    \"\"\"Appliquer ObsTimeExtensionWrapper et enlever la première feature.\"\"\"\n",
    "    return ObsTimeExtensionWrapper(feature_filter_wrapper_1(env))\n",
    "\n",
    "def obs_time_extension_wrapper_dtheta(env):\n",
    "    \"\"\"Appliquer ObsTimeExtensionWrapper et enlever la troisième feature.\"\"\"\n",
    "    return ObsTimeExtensionWrapper(feature_filter_wrapper_3(env))\n",
    "\n",
    "def obs_time_extension_wrapper_both(env):\n",
    "    \"\"\"Appliquer ObsTimeExtensionWrapper et enlever les deux features.\"\"\"\n",
    "    return ObsTimeExtensionWrapper(feature_filter_wrapper_both(env))\n",
    "\n",
    "def action_time_extension_wrapper_dx(env):\n",
    "    \"\"\"Appliquer ActionTimeExtensionWrapper et enlever la première feature.\"\"\"\n",
    "    return ActionTimeExtensionWrapper(feature_filter_wrapper_1(env))\n",
    "\n",
    "def action_time_extension_wrapper_dtheta(env):\n",
    "    \"\"\"Appliquer ActionTimeExtensionWrapper et enlever la troisième feature.\"\"\"\n",
    "    return ActionTimeExtensionWrapper(feature_filter_wrapper_3(env))\n",
    "\n",
    "def action_time_extension_wrapper_both(env):\n",
    "    \"\"\"Appliquer ActionTimeExtensionWrapper et enlever les deux features.\"\"\"\n",
    "    return ActionTimeExtensionWrapper(feature_filter_wrapper_both(env))\n",
    "\n",
    "def full_extension_wrapper_dx(env):\n",
    "    \"\"\"Appliquer ObsTimeExtensionWrapper, ActionTimeExtensionWrapper et enlever la première feature.\"\"\"\n",
    "    return ObsTimeExtensionWrapper(ActionTimeExtensionWrapper(feature_filter_wrapper_1(env)))\n",
    "\n",
    "def full_extension_wrapper_dtheta(env):\n",
    "    \"\"\"Appliquer ObsTimeExtensionWrapper, ActionTimeExtensionWrapper et enlever la troisième feature.\"\"\"\n",
    "    return ObsTimeExtensionWrapper(ActionTimeExtensionWrapper(feature_filter_wrapper_3(env)))\n",
    "\n",
    "def full_extension_wrapper_both(env):\n",
    "    \"\"\"Appliquer ObsTimeExtensionWrapper, ActionTimeExtensionWrapper et enlever les deux features.\"\"\"\n",
    "    return ObsTimeExtensionWrapper(ActionTimeExtensionWrapper(feature_filter_wrapper_both(env)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib backend: module://matplotlib_inline.backend_inline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969009056d124ab68ccbb1008e124fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_test={}\n",
    "fonction_wrappers_ici = obs_time_extension_wrapper_both\n",
    "ddpg_instance = DDPG(OmegaConf.create(params), [fonction_wrappers_ici])\n",
    "critic_losses, actor_losses, rewards_per_step, steps, best_rewards, running_rewwards =run_ddpg(ddpg_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.suptitle(f'Learning Curves for test', fontsize=16)\n",
    "\n",
    "# Tracer la perte des critiques\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(steps, critic_losses, label=f'Critic Losses test')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Critic Losses\")\n",
    "plt.legend()\n",
    "\n",
    "# Tracer la perte de l'acteur\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(steps, actor_losses, label=f'Actor Losses test')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Actor Losses\")\n",
    "plt.legend()\n",
    "\n",
    "# Tracer les récompenses par étape\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(steps, rewards_per_step, label=f'Rewards per Step test')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Rewards per Step\")\n",
    "plt.legend()\n",
    "\n",
    "# Tracer les meilleures récompenses\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(steps, running_rewwards, label=f'Best Rewards test')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Best Reward\")\n",
    "plt.title(\"Best Rewards\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniprojet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
